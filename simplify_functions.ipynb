{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "from transformers import OPTForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "\n",
    "TEACHER_TEMPLATE = \"\"\"\n",
    "Premise: {},\n",
    "Hypothesis: {},\n",
    "Label:{}\n",
    "\"\"\"[:-1]\n",
    "\n",
    "STUDENT_TEMPLATE = \"\"\"\n",
    "Label if this is entailment or contradiction.\n",
    "Premise: {},\n",
    "Hypothesis: {},\n",
    "Label:\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "def json_pretty(json_string):\n",
    "    print(json.dumps(json_string,indent=4))\n",
    "    return None\n",
    "\n",
    "def create_extended_prompt(context_examples, query_example):\n",
    "    query_example = deepcopy(query_example)\n",
    "    query_example['label'] = None\n",
    "    all_prompts = context_examples[:]\n",
    "    all_prompts.append(query_example)\n",
    "    processed_prompts = []\n",
    "    for prompt in all_prompts:\n",
    "        premise, hypothesis = prompt['premise'], prompt['hypothesis']\n",
    "        label = \"\"\n",
    "        if prompt['label'] is not None:\n",
    "            label = \" \" + LABELS[prompt['label']]\n",
    "        processed_prompt = TEACHER_TEMPLATE.format(premise,hypothesis,label)\n",
    "        processed_prompts.append(processed_prompt)\n",
    "    final_prompt = \"\\n\".join(processed_prompts)\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer + Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\") # tokenizer\n",
    "teacher_model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\") # teacher model\n",
    "student_model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\") # student model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"mnli\", split='validation_matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_examples = [{key: value[i] for key, value in dataset[5:7].items()} for i in range(2)]\n",
    "query_example = {key: value for key, value in dataset[7].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"premise\": \"well that would be a help i wish they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be\",\n",
      "        \"hypothesis\": \"We have plenty of space in the landfill.\",\n",
      "        \"label\": 2,\n",
      "        \"idx\": 5\n",
      "    },\n",
      "    {\n",
      "        \"premise\": \"yeah i know and i did that all through college and it worked too\",\n",
      "        \"hypothesis\": \"I did that all through college but it never worked \",\n",
      "        \"label\": 2,\n",
      "        \"idx\": 6\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_pretty(context_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"premise\": \"Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\",\n",
      "    \"hypothesis\": \"Most of Mrinal Sen's work can be found in European collections.\",\n",
      "    \"label\": 1,\n",
      "    \"idx\": 7\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_pretty(query_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Premise: well that would be a help i wish they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be,\n",
      "Hypothesis: We have plenty of space in the landfill.,\n",
      "Label: contradiction\n",
      "\n",
      "Premise: yeah i know and i did that all through college and it worked too,\n",
      "Hypothesis: I did that all through college but it never worked ,\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.,\n",
      "Hypothesis: Most of Mrinal Sen's work can be found in European collections.,\n",
      "Label:\n"
     ]
    }
   ],
   "source": [
    "teacher_prompt = create_extended_prompt(context_examples, query_example)\n",
    "student_prompt = STUDENT_TEMPLATE.format(query_example['premise'], query_example['hypothesis'])\n",
    "\n",
    "teacher_inputs = tokenizer(teacher_prompt, return_tensors=\"pt\")\n",
    "student_inputs = tokenizer(student_prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(teacher_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Premise: well that would be a help i wish they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be,\n",
      "Hypothesis: We have plenty of space in the landfill.,\n",
      "Label: contradiction\n",
      "\n",
      "Premise: yeah i know and i did that all through college and it worked too,\n",
      "Hypothesis: I did that all through college but it never worked,\n",
      "Label: contradiction\n",
      "\n",
      "Premise: Calcutta seems to be the only other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.,\n",
      "Hypothesis: Most of Mrinal Sen's work can be found in European collections.,\n",
      "Label: contradiction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_outputs = teacher_model.generate(**teacher_inputs, max_length=teacher_inputs['input_ids'].shape[-1] + 2)\n",
    "t_output_text = tokenizer.decode(teacher_outputs[0], skip_special_tokens=True)\n",
    "print(t_output_text)\n",
    "teacher_predicted_label = t_output_text.split(\"Label:\")[-1].strip().split('.')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " contradiction\n",
      "torch.Size([1, 186]) torch.Size([1, 50272])\n",
      "torch.Size([1, 50272])\n"
     ]
    }
   ],
   "source": [
    "teacher_outputs_1 = teacher_model.generate(**teacher_inputs, max_length=teacher_inputs['input_ids'].shape[-1] + 1, output_scores=True, return_dict_in_generate=True)\n",
    "teacher_probs_1 = torch.nn.functional.softmax(teacher_outputs_1.scores[0], dim=-1)\n",
    "print(tokenizer.decode(teacher_outputs_1[0][0][-1], skip_special_tokens=True))\n",
    "print(teacher_outputs_1[0].shape, teacher_outputs_1.scores[0].shape)\n",
    "\n",
    "argmax_index = torch.argmax(teacher_outputs_1.scores[0])\n",
    "print(teacher_probs_1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50272])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student_outputs = student_model.generate(**student_inputs, max_length=student_inputs['input_ids'].shape[-1] + 2)\n",
    "# s_output_text = tokenizer.decode(student_outputs[0], skip_special_tokens=True)\n",
    "# student_predicted_label = s_output_text.split(\"Label:\")[-1].strip().split('.')[0].strip()\n",
    "# print(s_output_text)\n",
    "\n",
    "# student_outputs = student_model.generate(**student_inputs, max_length=student_inputs['input_ids'].shape[-1] + 1, output_scores=True, return_dict_in_generate=True)\n",
    "# student_probs_1 = torch.nn.functional.softmax(student_outputs.scores[0], dim=-1)\n",
    "\n",
    "# print(tokenizer.decode(student_outputs[0][0][-1], skip_special_tokens=True))\n",
    "# print(student_outputs[0].shape, student_outputs.scores[0].shape)\n",
    "\n",
    "# s_argmax_index = torch.argmax(student_outputs.scores[0])\n",
    "# print(student_probs_1.shape, s_argmax_index)\n",
    "# student_probs_1.requires_grad\n",
    "\n",
    "student_logits = student_model(**student_inputs).logits \n",
    "student_probs_1 = torch.nn.functional.softmax(student_logits[:,-1,:], dim=-1)\n",
    "student_probs_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.3889, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=0.001)\n",
    "\n",
    "kl_divergence = torch.nn.functional.kl_div(student_probs_1.log(), teacher_probs_1, reduction='batchmean')\n",
    "\n",
    "print(kl_divergence)\n",
    "kl_divergence.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, Average Loss: 2.847226142883301\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "optimizer.zero_grad()\n",
    "kl_divergence.backward()\n",
    "optimizer.step()\n",
    "\n",
    "total_loss += kl_divergence.item()\n",
    "\n",
    "print(f\"1, Average Loss: {total_loss/len(query_example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
