{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OPTForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Premise: i don't know um do you do a lot of camping,\n",
    "            hypothesis: I know exactly,\n",
    "            label: Contradiction\n",
    "\n",
    "            Premise: This site includes a list of all award winners and a searchable database of Government Executive articles, \n",
    "            Hypothesis: The Government Executive articles housed on the website are not able to be searched, \n",
    "            Label: Contradiction\n",
    "\n",
    "            Premise: The new rights are nice enough, \n",
    "            Hypothesis: Everyone really likes the newest benefits, \n",
    "            Label: Entailment\n",
    "                        \n",
    "            Premise: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him\n",
    "            hypothesis: I like him for the most part, but would still enjoy seeing someone beat him very hard and enjoy that sight.\n",
    "            Label:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=inputs['input_ids'].shape[-1] + 10)\n",
    "output_text_ = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "label_out = output_text_.split(\"Label:\")[-1].strip().split('.')[0].strip()\n",
    "label_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    With MNLI tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"mnli\", split='validation_matched')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_text(label):\n",
    "    return [\"entailment\", \"neutral\", \"contradiction\"][label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_prompt(context_examples, query_example):\n",
    "    context_prompt = \"\"\n",
    "    for example in context_examples:\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = label_to_text(example['label'])\n",
    "        context_prompt += f\"Premise: {premise}, Hypothesis: {hypothesis}, Label: {label}\"\n",
    "\n",
    "    query_premise = query_example['premise']\n",
    "    query_hypothesis = query_example['hypothesis']\n",
    "    print('true lbl: ', label_to_text(query_example['label']))\n",
    "    query_prompt = f\"Premise: {query_premise}, Hypothesis: {query_hypothesis}, Label:\"\n",
    "    \n",
    "    return context_prompt + query_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_context(context_data, query_data, tokenizer, model):\n",
    "    prompt = create_extended_prompt(context_data, query_data)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs['input_ids'], max_length=inputs['input_ids'].shape[-1] + 1)\n",
    "    output_text = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    predicted_label = output_text.split(\"Label:\")[-1].strip().split('.')[0].strip()\n",
    "    return predicted_label\n",
    "\n",
    "context_examples = [{key: value[i] for key, value in dataset[:2].items()} for i in range(2)]\n",
    "query_example = {key: value for key, value in dataset[2].items()}\n",
    "\n",
    "print(context_examples)\n",
    "print(query_example)\n",
    "\n",
    "predicted_label = classify_with_context(context_examples, query_example, tokenizer, model)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
