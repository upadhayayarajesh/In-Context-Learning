{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2522581-45b7-49f3-9312-0259fd150dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q bitsandbytes accelerate loralib datasets\n",
    "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a51512-b462-44a4-ba11-1882b568c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import datasets as ds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from random import randint, sample\n",
    "\n",
    "from data_utils import (\n",
    "    load_glue_datasets,\n",
    "    load_hans_dataset,\n",
    "    load_paws_qqp_dataset,\n",
    "    get_dataset,\n",
    ")\n",
    "from context_utils import (\n",
    "    create_few_shot_context, \n",
    "    select_demonstrations, \n",
    "    create_train_batch_token,\n",
    "    create_validation_batch_token,\n",
    ")\n",
    "from training_utils import (\n",
    "    set_seed,\n",
    "    CastOutputToFloat,\n",
    "    get_model,\n",
    "    plot_losses,\n",
    "    train,\n",
    "    predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead6567c-cd31-4ad3-9562-7f5cbdb07d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a75959-3c9a-451f-a3e7-ad4fcadc7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     ds.logging.set_verbosity(40)\n",
    "\n",
    "# def print_trainable_parameters(model):\n",
    "#     trainable_params = 0\n",
    "#     all_param = 0\n",
    "#     for _, param in model.named_parameters():\n",
    "#         all_param += param.numel()\n",
    "#         if param.requires_grad:\n",
    "#             trainable_params += param.numel()\n",
    "#     print(\n",
    "#         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "#     )\n",
    "\n",
    "# class CastOutputToFloat(torch.nn.Sequential):\n",
    "#   def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "# def get_model(model_name):\n",
    "#     model_name = \"facebook/\" + model_name\n",
    "#     config = AutoConfig.from_pretrained(model_name)\n",
    "#     config.hidden_dropout_prob = 0.1\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenizer\n",
    "#     teacher_model = OPTForCausalLM.from_pretrained(model_name) # teacher model\n",
    "#     student_model = OPTForCausalLM.from_pretrained(model_name, config=config) # student model\n",
    "\n",
    "#     for param in student_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "#         if param.ndim == 1:\n",
    "#             param.data = param.data.to(torch.float32)\n",
    "    \n",
    "#     student_model.gradient_checkpointing_enable()\n",
    "#     student_model.enable_input_require_grads()\n",
    "#     student_model.lm_head = CastOutputToFloat(student_model.lm_head)\n",
    "\n",
    "#     config = LoraConfig(\n",
    "#         r=16,\n",
    "#         lora_alpha=32,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\"\n",
    "#     )\n",
    "    \n",
    "#     student_model = get_peft_model(student_model, config)\n",
    "#     return tokenizer, student_model, teacher_model\n",
    "\n",
    "# def get_dataset(data_set_used):\n",
    "#     datasets, labels, num_labels = load_glue_datasets(data_set_used)\n",
    "    \n",
    "#     if data_set_used in ['mnli', 'rte', 'hans']:\n",
    "#         teacher_prompt = 'Think logically. Are the following sentences examples of entailment, yes or no?'\n",
    "#         student_prompt = 'Are the following sentences examples of entailment, yes or no?'\n",
    "#     elif data_set_used in ['qqp', 'paws-qqp']:\n",
    "#         teacher_prompt = 'Think logically. Are the following sentences duplicates or paraphrases of each other, yes or no?'\n",
    "#         student_prompt = 'Are the following sentences duplicates or paraphrases of each other, yes or no?'\n",
    "\n",
    "#     return datasets, labels, num_labels, teacher_prompt, student_prompt\n",
    "    \n",
    "# # Assuming `losses` is the list of epoch losses returned from the `train` function\n",
    "# def plot_losses(losses):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(losses, marker='o', linestyle='-', color='b')\n",
    "#     plt.title('Training Loss Per Epoch')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Average Loss')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# def train(teacher_model, student_model, data, epochs=10, batch_size=32, device='cpu'):\n",
    "#     student_model.to(device)\n",
    "#     teacher_model.to(device)\n",
    "#     print(id(student_model))\n",
    "#     print(id(teacher_model))\n",
    "    \n",
    "#     student_model.train()\n",
    "#     optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-5, weight_decay=0.000001 )\n",
    "#     epoch_losses = []\n",
    "\n",
    "#     if (len(data) % batch_size != 0):\n",
    "#         num_batches = (len(data) // batch_size) + 1\n",
    "#     else:\n",
    "#         num_batches  = len(data) // batch_size\n",
    "\n",
    "#     num_datapoints = len(data)\n",
    "#     total_steps = num_batches * epochs\n",
    "#     warmup_ratio = int(0.1 * total_steps) \n",
    "    \n",
    "#     def lr_schedular(current_step: int):\n",
    "#         if current_step < warmup_ratio:\n",
    "#             return float(current_step) / float(max(1,warmup_ratio))\n",
    "#         return 1.\n",
    "    \n",
    "#     scheduler = LambdaLR(optimizer, lr_schedular)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         samples_left = num_datapoints\n",
    "        \n",
    "#         for i in range(num_batches):\n",
    "#             batch_loss = 0\n",
    "#             if ((samples_left - batch_size) >= 0):\n",
    "#                 samples_used = batch_size\n",
    "#                 samples_left -= batch_size\n",
    "#             else:\n",
    "#                 samples_used = samples_left\n",
    "#                 samples_left = 0\n",
    "#             for j in range(samples_used):\n",
    "#                 index = i * batch_size + j\n",
    "                \n",
    "#                 teacher_inputs = data[index]['context'].to(device)\n",
    "#                 student_inputs = data[index]['query'].to(device)\n",
    "                                \n",
    "#                 teacher_outputs = teacher_model.generate(\n",
    "#                     **teacher_inputs,\n",
    "#                     max_length=teacher_inputs['input_ids'].shape[-1] + 1,\n",
    "#                     output_scores=True,\n",
    "#                     return_dict_in_generate=True\n",
    "#                 )\n",
    "#                 teacher_probs = torch.nn.functional.softmax(teacher_outputs.scores[0], dim=-1)\n",
    "                \n",
    "#                 student_logits = student_model(**student_inputs).logits\n",
    "#                 student_probs = torch.nn.functional.softmax(student_logits[:, -1, :], dim=-1)\n",
    "                \n",
    "#                 kl_divergence = torch.nn.functional.kl_div(student_probs.log(), teacher_probs, reduction='batchmean')\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "#                 kl_divergence.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 batch_loss += kl_divergence.item()\n",
    "\n",
    "#             # Average loss for the batch\n",
    "#             batch_loss /= batch_size\n",
    "#             total_loss += batch_loss\n",
    "#             scheduler.step()\n",
    "#             #print(f\"Epoch {epoch + 1}, Batch {i + 1}, Average Loss: {batch_loss}\")\n",
    "\n",
    "\n",
    "#         # Average loss for the epoch\n",
    "#         epoch_loss = total_loss / num_batches\n",
    "#         epoch_losses.append(epoch_loss)\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}, Total Loss: {epoch_loss}\")\n",
    "        \n",
    "#     print(f\"Total loss : {total_loss/epochs}\")\n",
    "#     plot_losses(epoch_losses)\n",
    "#     # torch.save(student_model, './models/student.pth')\n",
    "\n",
    "# def predict(model, source, target=None, device='cpu'):\n",
    "#     predict = []\n",
    "#     for token in source:\n",
    "#         output = model.generate(**token, max_length=token['input_ids'].shape[-1] + 1).to(device)\n",
    "#         decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#         predicted_label = decoded_output.split(\"Label:\")[-1].strip().split('.')[0].strip()\n",
    "#         predict.append(predicted_label)\n",
    "        \n",
    "#     return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a750150-12ce-4d60-8cb9-3ef580899e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(dataset_used, model_name, epochs, val_len, train_len, context_len, seed):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    datasets, labels, num_labels, teacher_prompt, student_prompt = get_dataset(data_set_used)\n",
    "    \n",
    "    set_seed(seed)\n",
    "    print('starting run: {}'.format(seed))\n",
    "    print('loading model')\n",
    "    tokenizer, student_model, teacher_model = get_model(model_name)\n",
    "    print(\"finished loading model\")\n",
    "\n",
    "    print(\"loading data\")\n",
    "    train_data_tokens, train_data_strings, indices, context_indices = create_train_batch_token(\n",
    "        data_set_used, \n",
    "        datasets, \n",
    "        teacher_description = teacher_prompt, \n",
    "        student_description = student_prompt, \n",
    "        tokenizer=tokenizer, \n",
    "        seed=seed, \n",
    "        device=device,\n",
    "        num_shots = context_len,\n",
    "        num_train_samps=train_len,\n",
    "    )\n",
    "    print(\"finished loading data\")\n",
    "\n",
    "    print(\"training model\")\n",
    "    train(teacher_model, student_model, train_data_tokens, epochs = epochs, device=device)\n",
    "    print(\"finished training model\")\n",
    "\n",
    "    print(\"predicting on validation set\")\n",
    "    student_prompt_tokens, student_prompt_strings, val_indices, val_labels = create_validation_batch_token(\n",
    "        data_set_used, datasets, prompt_descr=student_prompt ,tokenizer=tokenizer, device=device, limit=100\n",
    "    )\n",
    "    prediction = predict(student_model, student_prompt_tokens, tokenizer = tokenizer, device=device)  \n",
    "    print(\"finished predicting on validation set\")\n",
    "\n",
    "    accuracy = accuracy_score(prediction,val_labels)\n",
    "    print(\"finished run {}\".format(seed))\n",
    "    print(\"final result\",accuracy)\n",
    "\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "        \n",
    "    meta_data_file_name = f'{dataset_used}_{model_name}_{seed}_{epochs}_{val_len}_{train_len}_{context_len}.json'\n",
    "    metadata_loc = os.path.join('output',meta_data_file_name)\n",
    "    metadata = {\n",
    "        'accuracy': accuracy,\n",
    "        'query_indices': indices,\n",
    "        'context_indices': context_indices.tolist(),\n",
    "        'validation_indices': val_indices.tolist(),\n",
    "        'model_name': model_name,\n",
    "        'dataset_used': dataset_used,\n",
    "        'seed': seed,\n",
    "        'epochs': epochs,\n",
    "        'val_len': val_len,\n",
    "        'train_len': train_len,\n",
    "        'context_len': context_len\n",
    "    }\n",
    "    \n",
    "    with open(metadata_loc, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd48760-4f90-44ad-a4dc-f0d738a3dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTED\n",
      "starting run: 0\n",
      "loading model\n",
      "finished loading model\n",
      "loading data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ee217ddd4e46958383f96021496c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a305e0646104d63a819a082aa9ea395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ffd8556d654599af99062624a404bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69889c70a98e485fafafec9b5469e361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f606f96d004d05a9fc293054899491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a63560d3c54e0597bfc871eac25762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07d992e9f4149a0b3de66a7792b136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fde1f835a31488fa36ca0da92f58e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f2a1f76034c85835878739914cc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e018a9dde0ea46ab9310ec0d8aeba644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d4f4f25d604b909a2cdea9c329c34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d71bc9067a74260899ad6b9d1fc3af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f9dc669f924ca6875dca632004f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8d1d5585ef4e19b7c75ebce71d0283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff67f5839ca14cb6ac10c7c1775bdba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bc9fdc0303449289c1b760e2519815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4255c97bf4fc411cbd789602fd8afc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbb3ae9566c49b1a8e10b35e583c7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runs = 10\n",
    "data_set_used = 'mnli'\n",
    "model_name = \"opt-125m\"\n",
    "epochs = 40\n",
    "val_len = 1000\n",
    "train_len = 1024\n",
    "context_len = 16\n",
    "\n",
    "print(\"STARTED\")\n",
    "for seed in range(runs):\n",
    "    run_job(data_set_used,model_name,epochs,val_len, train_len, context_len,seed)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d417f9-0537-4ea3-80a7-85517a6da87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
